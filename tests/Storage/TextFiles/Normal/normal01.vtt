WEBVTT

00:00:05.160 --> 00:00:10.100
Hello everyone and welcome to an introduction to linear regression and this lecture we're going to get

00:00:10.100 --> 00:00:15.600
a light theoretical background history behind the idea of linear regression before we actually tackle

00:00:15.600 --> 00:00:19.270
the concept with Python and the sikat learn library.

00:00:19.840 --> 00:00:23.020
If you want a deeper mathematical understanding of linear regression.

00:00:23.070 --> 00:00:28.460
You can go ahead and check out chapters 2 and 3 of an introduction to statistical learning.

00:00:29.610 --> 00:00:32.820
Let's discuss the history behind literary aggression.

00:00:32.820 --> 00:00:40.110
This entire idea all started back in the 1800s a man named Francis gult in Colten was studying the relationship

00:00:40.170 --> 00:00:45.780
between parents and their children and in particular he investigated the relationship between the heights

00:00:45.870 --> 00:00:49.260
of fathers and the heights of their sons.

00:00:49.710 --> 00:00:55.090
What he discovered was that a man's son tended to be roughly as tall as his father.

00:00:55.150 --> 00:01:01.740
However Galton's breakthrough was at the son's high tended to be closer to the overall average height

00:01:01.830 --> 00:01:04.530
of all people.

00:01:04.530 --> 00:01:06.920
Let's go ahead and take an example of this.

00:01:06.960 --> 00:01:10.630
Shaquille O'Neal was a famous NBA player is very tall.

00:01:10.680 --> 00:01:14.160
He's 7 foot 1 inch or 2.2 meters tall.

00:01:14.430 --> 00:01:19.670
If Shaq as he's known has a son chances are he'll be pretty tall too.

00:01:19.680 --> 00:01:25.560
However Czech is such an anomaly in height that there is also a very good chance that his son is not

00:01:25.560 --> 00:01:28.440
going to be as tall as Shaq.

00:01:29.060 --> 00:01:30.860
And it turns out this is the case.

00:01:30.990 --> 00:01:32.840
Shaquille O'Neal son is pretty tall.

00:01:32.850 --> 00:01:39.510
He's 6 foot seven inches but he's not nearly as tall as his dad who was 7 foot 1 Gault him calls this

00:01:39.510 --> 00:01:47.760
phenomenon regression as in a fathers sons Height's tends to regress or drift towards the mean or average

00:01:47.760 --> 00:01:50.640
height of everyone else.

00:01:50.670 --> 00:01:57.080
Let's go ahead and begin to plot out the sort of example let's cut to the regression with only two data

00:01:57.080 --> 00:02:00.040
points which is the simplest possible example.

00:02:00.120 --> 00:02:01.760
Here we have two data points.

00:02:01.940 --> 00:02:08.910
Sequels to and white was for as one data point and the X sequel's 5 and y equals 10 is another data

00:02:08.910 --> 00:02:09.460
point.

00:02:09.480 --> 00:02:16.440
These two little black dots all we're trying to do when we calculate our regression line is draw a line

00:02:16.440 --> 00:02:22.440
that's as close to every dot as possible for classic linear regression or the least squares method.

00:02:22.560 --> 00:02:25.920
You only measure the closeness in the up and down their action.

00:02:25.950 --> 00:02:31.340
Here we have a perfectly fitted line because we only had two points.

00:02:31.350 --> 00:02:36.970
Now wouldn't it be great if we could apply this same concept to grasp with more than just two data points

00:02:36.970 --> 00:02:37.400
.

00:02:37.590 --> 00:02:42.780
By doing this we could take multiple men and their sons heights and do things like tell Amanda how tall

00:02:42.780 --> 00:02:46.130
we expect the son to be before he even has a son.

00:02:46.320 --> 00:02:48.700
And this is the idea behind supervised learning.

00:02:48.840 --> 00:02:52.740
We're going to have a bunch of labeled data points create a model.

00:02:52.740 --> 00:02:59.460
In this case in theory aggression and try to take unlabeled data such as a father's high and spit back

00:02:59.460 --> 00:03:03.910
out labeled data our prediction of the sun's height.

00:03:04.170 --> 00:03:09.540
Our goal if linear regression is to minimize the vertical distance between all the data points in our

00:03:09.540 --> 00:03:10.220
line.

00:03:10.440 --> 00:03:16.470
So in determining the best line we are attempting to minimize the distance between all the points and

00:03:16.650 --> 00:03:19.420
distance to our line.

00:03:19.530 --> 00:03:22.200
There are lots of actually different ways to minimize this.

00:03:22.200 --> 00:03:25.280
The sum of squares error some of absolute errors etc..

00:03:25.340 --> 00:03:31.080
But all of these methods have a general goal of minimizing the distance between your line and the rest

00:03:31.080 --> 00:03:33.210
of the data points.

00:03:33.210 --> 00:03:37.830
For example one of the most popular methods that we just described is the least squares method.

00:03:37.920 --> 00:03:44.250
Here we have a couple of blue data points along an x and y axes and we want to fit a linear regression

00:03:44.250 --> 00:03:44.770
line.

00:03:44.970 --> 00:03:50.100
And the question is how do we decide which line is the best fitting one we can go ahead and use the

00:03:50.100 --> 00:03:52.760
least squares method which we discussed earlier.

00:03:52.980 --> 00:04:00.180
This method is fitted by minimizing the sum of the squares of the residuals the residuals for an observation

00:04:00.300 --> 00:04:05.160
is the difference between the observation the y value and the fitted line.

00:04:05.610 --> 00:04:09.020
In this image the residuals are marked by the red line.

00:04:09.180 --> 00:04:13.420
The difference between the true data point in blue and your fitted model line.

00:04:13.500 --> 00:04:15.920
The black Bagenal line.

00:04:16.200 --> 00:04:18.050
Right in the next lecture.

00:04:18.050 --> 00:04:22.370
We're going to use sikat learned in Python to create a linear regression model.

00:04:22.530 --> 00:04:27.930
Then you'll have your own portfolio project exercise and afterwards we'll go over the solutions to that

00:04:27.930 --> 00:04:29.960
project.

00:04:30.030 --> 00:04:32.830
Thanks everyone and I'll see you in the next lecture.